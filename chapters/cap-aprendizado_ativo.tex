%% ------------------------------------------------------------------------- %%n
\chapter{Aprendizado Ativo}
\label{cap:aprendizado_ativo}

\todo{Melhorar muito essa parte! Falar das surveys bases que serviu para essa dissertacao e das conferencias recentes!}
A premissa básica do aprendizado computacional é que se possa aprender a partir dos dados, usando um conjunto de observações e que possamos descobrir uma função não conhecida que explique aquele fenômeno. O fato desta premissa ser tão ampla faz com que seja muito difícil encaixar em uma única estrutura. Por isso, temos muitos paradigmas diferentes que lidam com situações específicas [\cite{abu2012learning}]. Dentro disso, uma das maneiras mais comuns que a literatura parece dividir se dá nos três grandes pilares: aprendizado supervisionado, não supervisionado e semi-supervisionado [\cite{abu2012learning}]. 

Dentro da definição acima, o Aprendizado Ativo aproxima-se ao aprendizado Semi-Supervisionado. Isso porque ambos olham para o mesmo problema: trabalhar com nenhum ou poucos dados rotulados ao passo que seja possível obter um classificador. Essa problemática aparece, na realidade, principalmente quando a rotulação demandaria um alto preço, como um grande esforço humano e/ou de recursos [\cite{settles2012active}]. É difícil, porém, saber exatamente em qual paradigma o Aprendizado Ativo se encaixa. Apesar de alguns trabalhos colocarem o aprendizado ativo como um subgrupo do semi-supervisionado \todo{x,y,z}, dois dos principais trabalhos de Aprendizado Semi-Supervisionado e Aprendizado Ativo colocam ambos paradigmas como paralelos e correlacionados [\cite{settles2012active, zhu2006semi}]. A pesquisa de Olson [\cite{olsson2009literature}], focada em Linguagem Natural, aponta em outra direção, colocando o aprendizado ativo embaixo da estrutura de supervisionado. É difícil, portanto, encontrar um consenso em qual categoria este tipo se encontra.


\section{Definição do Aprendizado Ativo}
\label{sec:definicao}

Aprendizado Ativo pode ser caracterizado quando há, de alguma forma, um controle ativo sobre os dados que são selecionados para o treinamento [\cite{cohn1994improving}]. Essa fase é conhecida na literatura como a estratégia de seleção de queries [\cite{settles2014active}]. Os dados selecionados, por sua vez, são postos a um oráculo que pode definir a classe corretamente [\cite{olsson2009literature}]. É importante pontuar que essa seleção ocorre dentro de um cenário específico.. Há, portanto, diferentes cenários e estratégias que podem ser utilizadas [\cite{settles2014active}]. No final o objetivo é que se encontre um classificador com a menor quantidade de dados de treinamento possível e que ainda possua uma boa acurácia [\cite{dasgupta2011two}].

Persello e Bruzone [\cite{persello2012active}] definem, em seu trabalho, esse tipo de aprendizado através de uma quíntupla (G, Q, S, T, U), que possui os seguintes elementos: um classificador (G), um framework de seleção de query (Q), um oráculo (S), uma base de treinamento (T) e uma base de dados sem classes (U). \todo{colocar algoritmo do paper}

\section{Cenários}
\label{sec:cenarios}

Existem alguns cenários onde o Aprendizado Ativo poderá organizar os dados para, então, fazer a seleção de queries. Dentro desses, os três principais que podem ser considerados na literatura são: (i) membership query synthesis, (ii) stream-based selective sampling e (iii) pool-based sampling \cite{settles2014active}. A figura ~\ref{fig:ActiveLearningScenarios} abaixo sintetiza a ideia.


\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{figures/active_learning_scenarios.png}
  \caption{Cenários de Aprendizado Ativo (Settles, 2014)}
  \label{fig:ActiveLearningScenarios}
\end{figure}


\subsection{Membership Query Synthesis}
\label{sec:cenarios_membeship}

Uma das primeiras formas de se pensar na organização dos dados foi atráves do método de membership. Neste cenário, é proposto que o algoritmo crie exemplos sintéticos para serem enviados ao oráculo. Os primeiros trabalhos que utilizaram essa ideia datam de 1980 [\cite{shapiro1981algorithm, shapiro1982algorithmic, shapiro198algorithmic_2}] e há muitas formas de se fazer isso. Podemos, por exemplo, mudar a estrutura de uma imagem ou retirar partes dela. 

De uma forma geral o que pretende-se fazer com esse cenário é, na distribuição do espaço de features, criar exemplos representativos. O trabalho de [\cite{baum1992query}] é um bom exemplo pois tentam sintetizar uma amostra através de uma rede neural de 2 camadas. O que eles prendentem, então, é encontrar, em uma dicotomia, um hiperplano que esteja entre duas amostras distintas. A figura ~\ref{fig:LangBaum_GeometryQueryLearning} do trabalho de Lang e Baum representa essa ideia. O ponto m, no meio da intersecção entre $x_+$ e $x_-$ seria uma amostra representativa.

\begin{figure}
  \centering
  \includegraphics[width=.4\textwidth]{figures/lang_baum_geometry_query_learning.png}
  \caption{A geometria do Aprendizado por Consulta [\cite{baum1992query}]}
  \label{fig:LangBaum_GeometryQueryLearning}
\end{figure}

É importante ressaltar que há desafios nesse sentido quando temos um domínio de alta complexidade, como o caso de imagens de raio-x, por exemplo [\cite{angluin1988queries}]. Mesmo em casos que poderiam ser mais simples encontramos dificuldades. Uma das principais limitações, acontecem quando o oráculo é um humano. Na imagem ~\ref{fig:LangBaum_5vs9Example}, os autores, [\cite{baum1992query}], utilizaram da ideia acima para gerar exemplos sintéticos. O interessante é que, dependendo, de onde os pontos se enontravam, algumas imagens não possuiam nenhum significado. 

\begin{figure}
  \centering
  \includegraphics[width=.4\textwidth]{figures/lang_baum_5_vs_9_example.png}
  \caption{Exemplos Sintéticos sem Significado [\cite{baum1992query}]}
  \label{fig:LangBaum_5vs9Example}
\end{figure}

Apesar dessa limitação para oráculos humanos, o trabalho [\cite{king2004functional, king2009automation}] conseguiu utilizar eficientemente essa ideia para quando o oráculo é um robo. Além desse, há um trabalho que utilizou de Genrative Adversarial Networks (GAN) em conjunto com Aprendizado Ativo para criar exemplos [\cite{zhu2017generative}]. O interessante é que eles revisitaram o trabalho de Lang e Baum e conseguiram criar exemplos significativos no caso de digitos, conforme a figura ~\ref{fig:GAN_5_vs_9}. Entretanto, geraram amostras sem significado para fotos de caes e gatos. 

\begin{figure}
  \centering
  \includegraphics[width=.9\textwidth]{figures/generative_GAN_AL_5_vs_9.png}
  \caption{Esquerda: exemplo de \cite{baum1992query} revisitado e na foto à direita o exemplo gerado pela GAN. [\cite{zhu2017generative}]}
  \label{fig:GAN_5_vs_9}
\end{figure}


Há, ainda, outras iniciativas com esse cenário. No trabalho de [\cite{wang2015active}], por exemplo, utiliza-se o paradigma de criar exemplos sintéticos mas, ao final, utilizar exemplos da própria base de dados para serem enviados ao oráculo. A ideia é parecida com o que foi citado acima mas, ao invés de criar amostras, a proposta é de utilizar a posição deles e utilizar dos vizinhos, pois eles serão reconhecidos por um humano e tem grande chance de serem informativos. A figura ~\ref{fig:wang_2015_membership}  mostra essa ideia. 

\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth]{figures/wang_2015_membership.png}
  \caption{Exemplos representativos a partir da posição de amostras sintéticas [\cite{wang2015active}].}
  \label{fig:wang_2015_membership}
\end{figure}


\subsection{Stream-based Selective Sampling}
\label{sec:cenarios_selective_sampling}

Neste cenário permanece a premissa de que temos muitas amostras sem rótulos. No entanto, elas são sequencialmente escolhidas e, então, selecionadas por algum critério quantitativo para, assim, serem levadas ao oráculo. Quando as amostras são escolhidas não se sabe ainda se ela será selecionada. É através de alguma medida de incerteza, que será discutida nas próximas seções, que essa amostra será levada ao oráculo ou descartada. É interessante notar que se a distribuição dos dados for uniforme, não teremos qualquer vantagem sobre o cenário anterior. Entretanto, se for não-uniforme e desconhecida, saberemos que as consultas serão sensíveis a esse fato [\cite{settles2014active}]. A figura ~\ref{fig:settles_2014_selective_sampling} mostra o fluxo. 

Há algumas situações específicas pelas quais é interessante utilizar o cenário sequencial de amostras. A mais comum é quando temos, por exemplo, uma limitação no poder computacional ou de memória. Nesses casos torna-se inviável processar o conjunto de dados sem rotulação de uma única vez. Outro exemplo interessante é quando temos aplicações na web. Nesses casos, também chamado de Online Learning, é interessante que as amostras sejam escolhidas de maneira sequencial. O trabalho de [\cite{chu2011unbiased}], motivado pelos milhões de dados diários do Yahoo, mostra como esse cenário pode ser benéfico. 


\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth]{figures/settles_2014_selective_sampling.png}
  \caption{Fluxo do cenário de Selective Sampling [\cite{settles2014active}].}
  \label{fig:settles_2014_selective_sampling}
\end{figure}



\subsection{Pool-based Sampling}
\label{sec:cenarios_pool}

Dos três cenários conhecidos na literatura, o pool-based é o mais utilizado em casos reais, enquantos os anteriores são mais comuns em trabalhos teóricos. Diferentemente do cenário de selective sampling, onde uma das motivações era a falta de recursos, como poder computacional ou memória, aqui escolhemos, de inicio, todo o conjunto de dados. Assume-se, neste caso, que tenhamos um conjunto muito grande de dados sem rótulos e um pequeno conjunto de amostras rotuladas. Também temos que esse conjunto seja estático, embora isso não seja estritamente necessário. A maior diferença entre os dois é que enquanto o primeiro escolhe as amostras sequencialmente e, então, decide, o pool analisa todas as amostras e, através de uma medida de utilidade, seleciona [\cite{settles2014active}]. A imagem ~\ref{fig:settles_2014_pool}  demostra o fluxo.

\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth]{figures/settles_2014_pool.png}
  \caption{Fluxo do cenário de Pool-Based Sampling [\cite{settles2014active}].}
  \label{fig:settles_2014_pool}
\end{figure}

Existem muitos trabalhos práticos que utilizam esse cenário. Por exemplo: \todo{pesquisar por trabalhos que utilizam o cenário pool} textotexto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto



\section{Estratégias de Consulta}
\label{sec:query_strategy}

Todos os cenários de Aprendizado Ative discutidos antriormente envolvem avaliar a representatividade das amostras a serem escolhidas. Há muitas formas propostas de se fazer isso na literatura e, a seguir, temos as principais delas [\cite{settles2012active}]. É importante notar que em todas as estratégias teremos alguma medida quantificável para selecionar as amostras. Por vezes podemos ter estratégias diferentes que utilizam medidas iguais ou similares.




\subsection{Amostras Incertas} %uncertainty sampling 
\label{sec:amostras_incertas}

Amostras Incertas é uma das estratégias mais utilizadas em Aprendizado Ativo. Isso acontece provavelmente por ser muito intuitiva e de fácil implementação [\cite{settles2014active}]. A ideia básica é que precisamos encontrar exemplos que, por terem um alto grau de incerteza, serão os mais representativos. Ou seja, queremos descartar exemplos onde o classificador já possui uma alta probabilidade de acerto e focar nos exemplos mais incertos.  


\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{figures/settles_2014_uncertainty_sampling_example.png}
  \caption{Exemplo de Amostras Incertas [\cite{settles2014active}].}
  \label{fig:settles_2014_uncertainty_example}
\end{figure}

A figura ~\ref{fig:settles_2014_uncertainty_example} exemplifica bem a ideia exposta. Nela temos como classificador uma regressão logisitca treinada em (b) por 30 amostras aleatórias e em (c) por 30 amostras mais representativas. É perceptível, comparando com (a), que o classificador pelas amostras incertas está muito acertivo. Isso ocorre porque as amostras mais representativas estarão, nesse caso, próximas da linha vertical que separa os dois grupos de dados.

Apesar da ideia ser intuitiva precisamos encontrar uma forma de medir a incerteza das amostras. É importante notar que uma interpretação probabilística pode ajudar. Isso porque, quando colocamos nesse escopo, conseguimos generalizar e modelar essa ideia para uma enorme de casos. Sendo o $x^*_{A}$ a melhor consulta possível utilizando a medida $A$, podemos pontuar as três principais formas de medir a incerteza de uma amostra [\cite{settles2014active}]:

\textbf{Menos Confiante:} nessa medida estamos interessados nos exemplos que são mais prováveis de serem classificados de forma errada. Uma maneira de formular isso seria:

%%EQUACAO ARRUMAR!
X {LC} =  

\textbf{Margem:} similar a medida anterior, a margem utiliza da primeira e segunda maiores probabilidades de uma determinada classe. Intuitivamente, caso a margem fique muito alta, significa que o classificador não possui uma incerteza muito grande em relação à classe adotada. Ao contrário, caso a margem fique estreita, o calssificador não sabe ao certo a qual classe aquela amostra pertecen:

%%EQUACAO ARRUMAR!
X M = 

\textbf{Entropia:} leva em consideração todas as probabilidades que uma amostra pode ser classificada. Nesse sentido, a entropia pode ser entendida como uma medida de impureza. 

%%EQUACAO ARRUMAR!
Formula entropia

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{figures/settles_2014_uncertainty_medidas.png}
  \caption{Comparação entre as três medidas [\cite{settles2014active}].}
  \label{fig:settles_2014_uncertainty_medidas}
\end{figure}



Das três medidas acima, é importante notar que a primeira leva em consideração apenas a maior probabilidade, enquanto a segunda apenas as duas maiores. A última, por outro lado, considera todas as possíveis classificações de uma amostra. Por isso mesmo é a mais comumente utilizada [\cite{settles2014active}]. Uma forma forma de comparar as três medidas pode ser vista na figura ~\ref{fig:settles_2014_uncertainty_medidas} abaixo. Nela é possível ver o resultado das funções de utilidade como função da probabilidade de determinada classificação. Assim, percebemos em um exemplo dicôtomico, na parte superior, que sempre que a probabilidade da classificação for 0.5, teremos o maior resultado de utilidade daquela medida. Da mesma forma, na parte inferior, em um exemplo com três possíveis classificações, percebemos a mesma coisa. 
\todo{Fal Limitacoes}


\subsection{Espaço de Hipóteses} 
\label{sec:hypothesis_space}

Uma outra estratégia que pode ser utilizada é procurar amostras dentro do espaço de hipóteses [\cite{mitchell1978version, mitchell1982generalization}. Nessa estratégia trabalharemos, por exemplo, com mais de um classificador ou com configurações diferentes de um mesmo classificador. Essa ideia foi implementada no trabalho de [\cite{atlas1990training,cohn1994improving}\ e pode ser vista na figura ~\ref{fig:cohn_1994_hypothesis_space_example}. Nela temos dois tipos de classificações (0 ou 1) e quatro hipóteses diferentes. As áreas mais escuras, onde há a interseção das diferentes hipóteses, representam a região onde pode-se haver as amostras mais incertas. 

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/cohn_1994_hypothesis_space_example.png}
  \caption{Exemplo Espaço de Hipóteses [\cite{cohn1994improving}].}
  \label{fig:cohn_1994_hypothesis_space_example}
\end{figure}

Um exemplo mais canônico pode ser visto no trabalho de [\cite{dasgupta2011two}], onde temos quatro classificadores lineares e a região em rosa representa a área de incerteza. Nessa estratégia uma amostra só iria ser selecionada se estivesse nessa região.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{figures/dasgupta_two_faces_hypothesis_example.png}
  \caption{Exemplo Espaço de Hipóteses [\cite{dasgupta2011two}].}
  \label{fig:dasgupta_two_faces_hypothesis_example}
\end{figure}

É importante pontuar que, como mostrado no trabalho de [\cite{settles2014active}], a ideia de procurar no espaço de hipóteses possui duas frentes principais: (i) consultas por desacordo e (ii) consultas por comite [\cite{seung1992query}]. As duas utilizam o mesmo paradigma que é de reduzir o espaço de hipóteses, mas consideram premissas diferentes. O trabalho de [\cite{cohn1994improving}], por exemplo, utiliza de uma aproximação, onde temos duas redes neurais que estão nos extremos do espaço de hipóteses e quando há um desacordo entre as classificações encontramos uma região de interesse. Apesar de ambas serem muito similares, elas possuem premissas diferentes. No primeiro, por exemplo, temos que a noção de desacordo necessita ser mensurada em todo o espaço de hipóteses, ou pelo menos em extremos, e precisa ser uma mensuração binária. A consulta por comite, por outro lado, relaxa um pouco essas premissas. De qualquer forma, nos últimos anos, entende-se que todo paradigma que utiliza um comitê ou conjunto, $C =$ { $\theta^{(1)}$, ..., $\theta^{(C)}$}, de hipóteses é considerado como consulta por comitê [\cite{settles2012active, settles2014active}].


Assim como ocorre na estratégia anterior, é necessário que tenhamos como medir a incerteza entre as diversas hipóteses. Há, na literatura, duas formas principais de se fazer isso: 

\textbf{Entropia por Voto:} a abordagem sugerida por [\cite{dagan1995committee}] considera todas as possíveis classificações e o número de votos que receberam. 

%%EQUACAO ARRUMAR!
X {LC} =  

\textbf{Divergência de Kullback-Leibler (KL):} a abordagem sugerida por [\cite{mccallumzy1998employing}] utiliza de uma abordagem probabiblistica, vinda da divergência de KL.

%%EQUACAO ARRUMAR!
X {LC} =  


\subsection{Explorando a Estrutura dos Dados} 
\label{sec:explorando_estrutura_dados }

texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto 


\subsection{Amostras Incertas vs. Espaço de Hipóteses} 
\label{sec:minimizing_expected}

texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto 


\subsection{Outras Estratégias} 
\label{sec:minimizing_expected}

texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto 





\section{Áreas Correlatas}
\label{sec:other_areas}

texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto texto 